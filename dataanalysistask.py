# -*- coding: utf-8 -*-
"""DataAnalysisTask.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wFDU0InAjgcgC57ecJpBc-QTztJe2DgJ
"""

!pip install requests
!pip install beautifulsoup4

!pip install pandas requests beautifulsoup4 openpyxl

import pandas as pd


df = pd.read_excel('Input.xlsx')


df.head()

import requests
from bs4 import BeautifulSoup

def extract_article(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')


    title = soup.find('h1').get_text()


    article_text = ""
    for paragraph in soup.find_all('p'):
        article_text += paragraph.get_text() + "\n"

    return title, article_text

import os
import pandas as pd
import requests
from bs4 import BeautifulSoup

def extract_article(url):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')


        title = soup.find('h1').get_text(strip=True)


        article = soup.find('article')
        if article:
            text = ' '.join(p.get_text(strip=True) for p in article.find_all('p'))
        else:

            text = ' '.join(p.get_text(strip=True) for p in soup.find_all('p'))

        return title, text
    except Exception as e:
        print(f"Error extracting article from {url}: {e}")
        return None, None


input_excel = '/content/Input.xlsx'


if os.path.exists(input_excel):
    df = pd.read_excel(input_excel)

    os.makedirs('/content/articles', exist_ok=True)


    for index, row in df.iterrows():
        url_id = row['URL_ID']
        url = row['URL']


        title, text = extract_article(url)

        if title and text:

            try:
                with open(f'/content/articles/{url_id}.txt', 'w', encoding='utf-8') as file:
                    file.write(f"{title}\n\n{text}")
                print(f"Saved article {url_id}")
            except Exception as e:
                print(f"Failed to save {url_id}: {e}")
        else:
            print(f"Skipping {url_id} due to extraction error")

    print("Extraction and saving process completed.")
else:
    print(f"File '{input_excel}' not found.")

import pandas as pd
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.tokenize import word_tokenize, sent_tokenize
import re
from collections import Counter

nltk.download('punkt')
nltk.download('vader_lexicon')

def count_syllables(word):
    word = word.lower()
    vowels = "aeiouy"
    syllable_count = 0
    if word[0] in vowels:
        syllable_count += 1
    for index in range(1, len(word)):
        if word[index] in vowels and word[index - 1] not in vowels:
            syllable_count += 1
    if word.endswith("e"):
        syllable_count -= 1
    if word.endswith("le") and len(word) > 2:
        syllable_count += 1
    if syllable_count == 0:
        syllable_count = 1
    return syllable_count


def analyze_text(text):
    words = word_tokenize(text)
    sentences = sent_tokenize(text)



    word_count = len(words)
    sentence_count = len(sentences)



    avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0


    complex_words = [word for word in words if count_syllables(word) >= 3]
    complex_word_count = len(complex_words)



    percentage_complex_words = (complex_word_count / word_count) * 100 if word_count > 0 else 0



    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)



    avg_words_per_sentence = word_count / sentence_count if sentence_count > 0 else 0


    avg_word_length = sum(len(word) for word in words) / word_count if word_count > 0 else 0


    syllables_per_word = sum(count_syllables(word) for word in words) / word_count if word_count > 0 else 0


    personal_pronouns = re.findall(r'\b(I|we|my|ours|us)\b', text, re.I)
    personal_pronoun_count = len(personal_pronouns)


    sid = SentimentIntensityAnalyzer()
    sentiment_scores = sid.polarity_scores(text)
    positive_score = sentiment_scores['pos']
    negative_score = sentiment_scores['neg']
    polarity_score = sentiment_scores['compound']
    subjectivity_score = (positive_score + negative_score) / (word_count if word_count > 0 else 1)

    return {
        "Word Count": word_count,
        "Sentence Count": sentence_count,
        "Positive Score": positive_score,
        "Negative Score": negative_score,
        "Polarity Score": polarity_score,
        "Subjectivity Score": subjectivity_score,
        "Average Sentence Length": avg_sentence_length,
        "Percentage of Complex Words": percentage_complex_words,
        "Fog Index": fog_index,
        "Average Number of Words per Sentence": avg_words_per_sentence,
        "Complex Word Count": complex_word_count,
        "Syllables per Word": syllables_per_word,
        "Personal Pronouns": personal_pronoun_count,
        "Average Word Length": avg_word_length
    }


results = []


articles_dir = 'articles'


for file_name in os.listdir(articles_dir):
    if file_name.endswith('.txt'):
        url_id = file_name.split('.')[0]
        file_path = os.path.join(articles_dir, file_name)


        with open(file_path, 'r', encoding='utf-8') as file:
            title = file.readline().strip()
            text = file.read()


        analysis = analyze_text(text)


        result = {
            "URL_ID": url_id,
            "Title": title,
            **analysis
        }

        results.append(result)


df_results = pd.DataFrame(results)


columns = [
    "URL_ID", "Title", "Positive Score", "Negative Score", "Polarity Score",
    "Subjectivity Score", "Average Sentence Length", "Percentage of Complex Words",
    "Fog Index", "Average Number of Words per Sentence", "Complex Word Count",
    "Word Count", "Syllables per Word", "Personal Pronouns", "Average Word Length"
]


df_results = df_results[columns]



df_results.to_excel('output.xlsx', index=False)


df_results.head()